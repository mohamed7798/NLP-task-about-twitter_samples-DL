{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5744eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kimoa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\kimoa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout,Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import twitter_samples\n",
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c41d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "tweets = positive_tweets + negative_tweets\n",
    "labels = np.append(np.ones(len(positive_tweets)), np.zeros(len(negative_tweets)))\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d2df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    # Remove URLs and mentions\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+|\\@\\w+\", \"\", tweet)\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    tweet = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", tweet).lower()\n",
    "    # Tokenize the tweet into words\n",
    "    words = tweet.split()\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    # Convert all words to string\n",
    "    cleaned_tweet = ' '.join(str(word) for word in words)\n",
    "    return cleaned_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a72d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "for tweet in tweets:\n",
    "    cleaned_tweets.append(clean_tweets(tweet))\n",
    "\n",
    "# Convert the list of words to sequences of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_tweets)\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_tweets)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_len = 30\n",
    "padded_sequences = np.zeros((len(sequences), max_len), dtype=int)\n",
    "for i, sequence in enumerate(sequences):\n",
    "    j = max_len - len(sequence)\n",
    "    padded_sequence_str = ' '.join(map(str, sequence[:max_len-j]))\n",
    "    padded_sequences[i, j:] = np.array(list(map(int, padded_sequence_str.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400c1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_size = 32\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0960eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59cb0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27de00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92f54dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.6822 - accuracy: 0.6022 - val_loss: 0.6578 - val_accuracy: 0.7180\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.5742 - accuracy: 0.8145 - val_loss: 0.5425 - val_accuracy: 0.7430\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.4095 - accuracy: 0.8756 - val_loss: 0.4883 - val_accuracy: 0.7605\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.2966 - accuracy: 0.9124 - val_loss: 0.4818 - val_accuracy: 0.7565\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.2220 - accuracy: 0.9362 - val_loss: 0.4892 - val_accuracy: 0.7545\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1717 - accuracy: 0.9521 - val_loss: 0.5054 - val_accuracy: 0.7485\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.1371 - accuracy: 0.9621 - val_loss: 0.5243 - val_accuracy: 0.7450\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.1128 - accuracy: 0.9681 - val_loss: 0.5436 - val_accuracy: 0.7410\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 0.0951 - accuracy: 0.9734 - val_loss: 0.5666 - val_accuracy: 0.7360\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.0820 - accuracy: 0.9783 - val_loss: 0.5863 - val_accuracy: 0.7350\n"
     ]
    }
   ],
   "source": [
    "# Reshape the ground truth labels\n",
    "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(padded_sequences, y_train, batch_size=64, epochs=10)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84092fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 2s 8ms/step - loss: 0.6780 - accuracy: 0.6060\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.5373 - accuracy: 0.8152\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.3725 - accuracy: 0.8792\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.2699 - accuracy: 0.9161\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.2024 - accuracy: 0.9363\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.1571 - accuracy: 0.9527\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.1267 - accuracy: 0.9628\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.1052 - accuracy: 0.9693\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0896 - accuracy: 0.9738\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0779 - accuracy: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c84b636340>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Convert the list of words to sequences of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_tweets)\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_tweets)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_len = 30\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, truncating='post')\n",
    "\n",
    "# Define the Keras model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, batch_size=64, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4adfaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07036126405000687\n",
      "Test accuracy: 0.9779999852180481\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650423d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 3s 11ms/step - loss: 0.6767 - accuracy: 0.5978 - val_loss: 0.6118 - val_accuracy: 0.8200\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.5330 - accuracy: 0.8173 - val_loss: 0.3997 - val_accuracy: 0.8785\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.3714 - accuracy: 0.8745 - val_loss: 0.2824 - val_accuracy: 0.9155\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.2712 - accuracy: 0.9140 - val_loss: 0.2130 - val_accuracy: 0.9365\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.2048 - accuracy: 0.9376 - val_loss: 0.1648 - val_accuracy: 0.9520\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.1601 - accuracy: 0.9505 - val_loss: 0.1326 - val_accuracy: 0.9615\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.1290 - accuracy: 0.9615 - val_loss: 0.1105 - val_accuracy: 0.9685\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.1070 - accuracy: 0.9677 - val_loss: 0.0954 - val_accuracy: 0.9710\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0914 - accuracy: 0.9725 - val_loss: 0.0827 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0795 - accuracy: 0.9757 - val_loss: 0.0732 - val_accuracy: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c84bc6e0a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Convert the list of words to sequences of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_tweets)\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_tweets)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_len = 30\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, truncating='post')\n",
    "\n",
    "# Define the Keras model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, batch_size=64, epochs=epochs, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "784dbbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07320137321949005\n",
      "Test accuracy: 0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719802e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
